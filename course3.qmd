---
title: "Course Three"
toc: true
number-sections: true
---

## Data Integration with Microsoft Azure Data Factory

__Syllabus__

### Week 1
Integrate data with Azure Data Factory or Azure Synapse Pipeline
Describe the core components of Azure Data Factory that enable you to create large scale data ingestion solutions in the cloud
9 videos, 7 readings, 2 practice quizzes

Graded: Test prep

### Week 2
Petabyte-scale ingestion with Azure Data Factory or Azure Synapse Pipeline
Describe the various methods that can be used to ingest data between various data stores using Azure Data Factory
5 videos, 4 readings, 1 practice quiz

Graded: Test prep

### Week 3
Perform code-free transformation at scale with Azure Data Factory or Azure Synapse Pipeline
Perform common data transformation and cleansing activities within Azure Data Factory without using code Implement Slowly Changing Dimension using Azure Data Factory or Azure Synapse Pipelines
7 videos, 7 readings, 4 practice quizzes

Graded: Test prep

This weeks material is quite useful.  Summary below:

__Lesson introduction__

Video•. Duration: 1 minute1 min
Azure Data Factory transformation methods

Ungraded Plugin•. Duration: 15 minutes15 min
Explain Azure Data Factory transformation methods

Video•. Duration: 2 minutes2 min
Azure Data Factory transformation types

Reading•. Duration: 5 minutes5 min
Exercise: Author an Azure Data Factory mapping data flow

Reading•. Duration: 20 minutes20 min
Exercise quiz

Practice Quiz•1 question
Debug mapping data flow

Video•. Duration: 1 minute1 min
Use Azure Data Factory wrangling data

Reading•. Duration: 8 minutes8 min
Example - Use compute transformations within Azure Data Factory

Reading•. Duration: 15 minutes15 min
Integrate SQL server integration services packages within Azure Data Factory

Reading•. Duration: 15 minutes15 min
Knowledge check

Practice Quiz•6 questions
Lesson summary

Video•. Duration: 1 minute1 min
Additional resources

Reading•. Duration: 3 minutes3 min

__Populate slowly Changing Dimensions in Azure Synapse Analytics Pipeline__

Describe slowly changing dimensions

Video•. Duration: 3 minutes3 min
Choose between slowly changing dimension types

Video•. Duration: 4 minutes4 min
Exercise - Design and implement a Type 1 slowly changing dimension with mapping data flows

https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/

Reading•. Duration: 18 minutes18 min
Exercise quiz

Practice Quiz•1 question
Knowledge check

Practice Quiz•5 questions
Test prep

Due, Jul 23, 11:59 PM PDT
Quiz•9 questions
•Grade: --

Lesson summary

Video•. Duration: 1 minute1 min

### Week 4
Orchestrate data movement and transformation in Azure Data Factory or Azure Synapse Pipeline
Use Azure Data Factory to orchestrate large scale data movement by using other Azure Data Platform and Machine Learning technologies
6 videos, 1 reading, 2 practice quizzes

Graded: Test prep

### Week 5
Execute existing SSIS packages in Azure Data Factory or Azure Synapse Pipeline
Integrate SQL Server Integration Services packages into an Azure Data Factory solution
3 videos, 3 readings, 1 practice quiz

Graded: Test prep

### Week 6
Operationalize your Azure Data Factory or Azure Synapse Pipeline
Publish your Azure Data Factory work between different environments
3 videos, 6 readings, 1 practice quiz
expandweek 6 material

Graded: Test prep

### Week 7
Receive data with Azure Data Share and transforming with Azure Data Factory
Ingest data from Azure Data Share into Azure Data Factory pipelines to build automated ingestion pipelines
6 videos, 6 readings, 6 practice quizzes

Graded: Test prep

### Week 8
Course Practice Exam
Practice Exam on Data integration at scale with Azure Data Factory or Azure Synapse Pipeline
2 videos, 2 readings

Graded: Course practice exam


## Useful Tutorials/Resources
### Explore Azure Synapse
https://microsoftlearning.github.io/dp-203-azure-data-engineer/Instructions/Labs/01-Explore-Azure-Synapse.html

## Connectors
Notes:
Connectors are Azure Data Factory objects that enable your Linked Services and Datasets to connect to a wide variety of data sources and sinks. These can include connections to Azure resources and third-party connectors such as Amazon S3 or Google cloud. There are nearly 100 connectors that are available, and they work with the Copy, Data Flow, Look up, Get Metadata, and Delete activities that can be found within Azure Data Factory.

## File Formats


### File Format List
- Avro format
- Binary format
- Delimited text format
- JSON format
- ORC format
- Parquet format

There are too many data stores to list, but the following table lists the categories of data stores and two examples of the types of connectors that exist

## Data Stores
Category	Data Store example
Azure	Azure Data Lake Store, Azure Synapse Analytics
Databases	Netezza, Greenplum
NoSQL stores	Cassandra, MongoDB
File	FTP, Google Cloud Storage
Generic protocols	REST, ODBC
Services & Apps	Dynamics, SalesForce
The list of connectors is constantly evolving. You can keep up to date with the latest list, and the activity support by looking at the connectors overview page

#### Azure Data Factory
https://learn.microsoft.com/en-us/azure/data-factory/author-visually?tabs=data-factory

#### ADF Source Control
https://learn.microsoft.com/en-us/azure/data-factory/source-control


## Azure Play Projects

https://github.com/MikeBeaulieuQTech/adf-cicd

*Summary*
Created two storaged locations and copied a pgn file from on to the other.

In order to utilize the pgn it can either be read in an app (PGN Reader) or it can be processed in python.
Next steps will be to write a script to process the file.
Some tasks:
a. create an image that shows the entire game
b. load the moves and commentary into a model that can be persisted in a database.

I did a bit of research into loading the file from a storage location into a jupyter notebook but I haven't completed this task - I don't think it's the best way, but it might be a useful pre-analyis task to know how to do.

